# -*- coding: utf-8 -*-
"""reddotfinder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RxNaXg_TTTr2ImDApeVKo86XDWU2cMNy
"""

#Importing Libraries
import cv2
import numpy as np
from scipy import signal
import matplotlib.pyplot as plt
from google.colab.patches import cv2_imshow

##Imports Drive
from google.colab import drive
drive.mount('/content/drive')

##Function that filters out the led position using Thresholds and finds the COM of the contour
def _led_position_1(img,threshold_value):

  #Image Test
      
  grey = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
  blur = cv2.GaussianBlur(grey,(5,5),0)
  thresh_grey = cv2.threshold(blur, threshold_value, 255, cv2.THRESH_BINARY)[1]
    # #Bitwise-AND mask and original image
  res = cv2.bitwise_and(img,img,mask= thresh_grey)

    # #BGR2Gray Conversion so we can use the minMaxLocation function
  grayres = cv2.cvtColor(res, cv2.COLOR_BGR2GRAY)

  minVal, maxVal, minLoc, maxLoc =cv2.minMaxLoc(grayres)
  M = cv2.moments(thresh_grey)
    # calculate x,y coordinate of center
  if(M["m00"] == 0):
    cX = 0
    cY = 0
  else:
    cX = int(M["m10"] / M["m00"])
    cY = int(M["m01"] / M["m00"])

    # put text and highlight the center
    # display the image
  cv2.destroyAllWindows()
  return cX,cY

#Butterworth Lowpass Filter that takes the array and threshold and returns a filtered dataset
def _butterworthLOWPASS_(thr,arr):
  #Default Order
  n = 1
  Fs = 1

  #Constructing the filter
  nyquist_freq = Fs/2
  Wn = thr/nyquist_freq
  b,a = signal.butter(n,thr,'low',analog = False,output = 'ba') #lowpass butterworth filter using threshold
  arrfilt = signal.filtfilt(b,a,arr,0) #zerophase filter using coefficients

  return arrfilt

##Capturing Video Function that takes the video needing to be analyzed and returns a labeled video with a red marker on the LED and a set of datapoints corresponding to the LED
##Feelfree to change the threshold of the smooothener if its not smooth enough
def videoandmatlabfunction(_videofilepath_,threshold_value = 247):
  cap = cv2.VideoCapture(_videofilepath_)
  vlength = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) ##total number of frames

  x = np.zeros((vlength,1),dtype = int)
  y = np.zeros((vlength,1),dtype = int)

  i = 0
  while(cap.isOpened()):
      if (i%1000 ==0):
        print(str((i*100)/vlength)+'% Frames Done-LEDPOSITION')
      ret, crop = cap.read()
      if(ret == True):

        #crop = frame[10:455, 10:455] #Cropping out frame box because the light sometimes reflects on the edges of the box and throws off detector

        height,width,channels = crop.shape
        ledy1,ledx1 = _led_position_1(crop,threshold_value)
        if(ledx1 == 0 or ledy1 == 0):
          x[i] = x[i-1]
          y[i] = y[i-1]
        else:
          x[i] = int((ledx1))
          y[i] = int((ledy1))
        i = i+1
        if cv2.waitKey(1) & 0xFF == ord('q'):
                  break
      else:
        break

  #"""" Butterworth LowPass Filter for smoothening out coordinate frame analysis (lower j)""""
  threshold = 0.3
  filtx = _butterworthLOWPASS_(threshold,x)
  filty = _butterworthLOWPASS_(threshold,y)
  plt.plot(filty,filtx)
  plt.xlabel('X (Pixels)')
  plt.ylabel('Y (Pixels)')
  plt.title('Mappings')

  #Cv2 writer that writes the output to a file (Change the first input to the name of the file)
  outputVideo = 'Labeled_.avi'
  outputData = 'Labeled_.mat'
  cap1 = cv2.VideoWriter(outputVideo,cv2.VideoWriter_fourcc('M','J','P','G'), 20, (width,height))
  cap.set(1,0)
  i = 0
  while(cap.isOpened()):
        ret,crop = cap.read()
        if(ret == True):
          #crop = frame[10:455, 10:455] #Cropping out frame box because the light sometimes reflects on the edges of the box and throws off detector
          cv2.circle(crop,(filty[i],filtx[i]), 4, (0,0,255),2)
          #cv2_imshow(frame)
          cap1.write(crop)
          i=i+1
          #  if cv2.waitKey(1) & 0xFF == ord('q'):
          #         break
          if(i%1000==0):
            print(str((i*100)/vlength) + '% Frame Done')

        else:
          break    
  arr = np.column_stack((filtx,filty))
  scipy.io.savemat(outputData, mdict={'arr': arr})
  cap1.release()
  cap.release()
  cv2.destroyAllWindows()

##Insert your path file to the videos , The function will iterate through the videos and download the video and matlab files by name
import scipy.io
from google.colab import files
directory_video = '/content/drive/My Drive/D1_854 soc'
videofiletype = '.avi'
import os

filelist = os.listdir(directory_video)
desired_threshold = 240 ##Threshold for evaluating values, Raise it (~247 )if it seems to be a little off, decrease to (~240) if the tracker isnt picking it up


for infile in sorted(filelist):
  sample_cap = cv2.VideoCapture(directory_video + '/' +str(infile))
  (width,height)=(int(sample_cap.get(3)),int(sample_cap.get(4)))
  
fullVideo = 'Unlabeled_Video.avi' #Name for Unlabeled Combined Video
cap1 = cv2.VideoWriter(fullVideo,cv2.VideoWriter_fourcc('M','J','P','G'), 20,(width,height))
for infile in sorted(filelist, key=lambda x: int("".join([i for i in x if i.isdigit()]))): 
  print ('Combining:'+directory_video + '/' +str(infile))
  cap = cv2.VideoCapture(directory_video + '/' +str(infile))
  while(cap.isOpened()):
    ret,frame = cap.read()
    if(ret == True):
      cap1.write(frame)
    else:
      break
  cap.release()
  
cap1.release()
videoandmatlabfunction(fullVideo,desired_threshold)
cv2.destroyAllWindows()